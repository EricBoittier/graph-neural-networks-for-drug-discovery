net: ENNS2V
args: {model: ENNS2V, cuda: False, train_set: data/train.csv.gz, valid_set: data/validate.csv.gz, test_set: data/test.csv.gz, loss: MaskedMultiTaskCrossEntropy, score: roc-auc, epochs: 20, batch_size: 50, learn_rate: 0.0001, savemodel: True, logging: more, message_passes: 5, message_size: 50, enn_depth: 3, enn_hidden_dim: 100, enn_dropout_p: 0.0, s2v_lstm_computations: 9, s2v_memory_size: 50, out_depth: 2, out_hidden_dim: 150, out_dropout_p: 0.0}
****** MODULES: ******
enn: FeedForwardNetwork(in_features=4, hidden_layer_sizes=[100, 100, 100], out_features=3750, activation=SELU(), bias=False, dropout_p=0.0)
gru: GRUCell(50, 75, bias=False)
s2v: Set2Vec(
  (embedding_matrix): Linear(in_features=150, out_features=50, bias=False)
  (lstm): LSTMCell(50, 50, bias=False)
)
out_nn: FeedForwardNetwork(in_features=100, hidden_layer_sizes=[150, 150], out_features=1, activation=SELU(), bias=False, dropout_p=0.0)
**********************
score metric: roc-auc
columns:
epochs, mean training score, mean validation score, mean testing score, best-model-so-far mean training score, best-model-so-far mean validation score, best-model-so-far mean testing score
1, 0.768701, 0.740668, 0.719605, 0.768701, 0.740668, 0.719605
2, 0.957147, 0.931403, 0.929150, 0.957147, 0.931403, 0.929150
3, 0.975573, 0.924246, 0.926215, 0.957147, 0.931403, 0.929150
4, 0.949339, 0.827749, 0.862585, 0.957147, 0.931403, 0.929150
5, 0.990556, 0.925064, 0.935307, 0.957147, 0.931403, 0.929150
6, 0.993863, 0.922195, 0.931745, 0.957147, 0.931403, 0.929150
7, 0.979707, 0.919230, 0.923366, 0.957147, 0.931403, 0.929150
8, 0.998676, 0.930870, 0.946133, 0.957147, 0.931403, 0.929150
9, 0.999416, 0.917110, 0.949904, 0.957147, 0.931403, 0.929150
10, 0.999681, 0.931493, 0.955885, 0.999681, 0.931493, 0.955885
11, 1.000000, 0.950553, 0.964721, 1.000000, 0.950553, 0.964721
12, 0.999593, 0.962338, 0.958797, 0.999593, 0.962338, 0.958797
13, 0.999993, 0.965705, 0.973002, 0.999993, 0.965705, 0.973002
14, 0.999980, 0.959927, 0.967646, 0.999993, 0.965705, 0.973002
15, 0.999823, 0.915406, 0.956350, 0.999993, 0.965705, 0.973002
16, 0.999925, 0.945246, 0.959807, 0.999993, 0.965705, 0.973002
17, 0.999993, 0.948758, 0.969754, 0.999993, 0.965705, 0.973002
18, 1.000000, 0.951301, 0.965349, 0.999993, 0.965705, 0.973002
19, 1.000000, 0.944650, 0.963468, 0.999993, 0.965705, 0.973002
20, 0.995960, 0.952770, 0.965395, 0.999993, 0.965705, 0.973002
